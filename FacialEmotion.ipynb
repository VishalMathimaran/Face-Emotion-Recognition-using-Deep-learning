{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import random\n",
    "import os\n",
    "\n",
    "from imutils import paths\n",
    "\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Activation\n",
    "from keras.preprocessing.image import img_to_array\n",
    "from keras.utils import to_categorical\n",
    "from keras.optimizers import Adam, SGD\n",
    "from keras.callbacks import EarlyStopping, TensorBoard, ModelCheckpoint\n",
    "from keras import applications\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.applications.vgg19 import VGG19\n",
    "from keras.applications.resnet50 import ResNet50\n",
    "from keras.applications.inception_v3 import InceptionV3\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, Dropout, Flatten, Conv2D, MaxPooling2D\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.metrics import categorical_accuracy\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import load_model\n",
    "from keras import backend as K\n",
    "from collections import defaultdict\n",
    "\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report, precision_recall_fscore_support, average_precision_score, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "path='data'\n",
    "data = []\n",
    "labels = []\n",
    "features = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_STATE = 42\n",
    "IMAGE_DIM = 48\n",
    "TEST_SPLIT = 0.20\n",
    "BATCH_SIZE = 50\n",
    "EPOCHS = 50\n",
    "INIT_LR = 1e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class information :  {'1': 0, '2': 1, '3': 2, '4': 3, '5': 4, '6': 5, '7': 6}\n"
     ]
    }
   ],
   "source": [
    "class_dir_paths = sorted(os.listdir(path))\n",
    "class_dict = {}\n",
    "idx = 0\n",
    "\n",
    "for class_dir_path in class_dir_paths:\n",
    "    if (os.path.isdir(os.path.join(path, class_dir_path))):\n",
    "        class_name = class_dir_path\n",
    "        class_dict[class_name] = idx\n",
    "        idx += 1\n",
    "\n",
    "print(\"Class information : \", class_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<module 'imutils.paths' from '/home/tennyson/.local/lib/python3.6/site-packages/imutils/paths.py'>\n"
     ]
    }
   ],
   "source": [
    "imagePaths = sorted(list(paths.list_images(path)))\n",
    "random.seed(RANDOM_STATE)\n",
    "random.shuffle(imagePaths)\n",
    "print(paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of images readed,  1923\n",
      "Total number of labels extracted,  1923\n"
     ]
    }
   ],
   "source": [
    "# loop over the input images\n",
    "for imagePath in imagePaths:\n",
    "    # load the image, pre-process it, and store it in the data list\n",
    "    image = cv2.imread(imagePath)\n",
    "    image = cv2.resize(image, (IMAGE_DIM, IMAGE_DIM))\n",
    "    image = img_to_array(image)\n",
    "    feature = image_to_feature_vector(image)\n",
    "    data.append(image)\n",
    "    features.append(feature)\n",
    "\n",
    "    # extract the class label from the image path and update the\n",
    "    # labels list\n",
    "    label = imagePath.split(os.path.sep)[-2]\n",
    "    labels.append(class_dict[label])\n",
    "\n",
    "print(\"Total number of images readed, \", len(data))\n",
    "print(\"Total number of labels extracted, \", len(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1923"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(imagePaths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1923, 48, 48, 3)"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = np.array(data, dtype=\"float\")/255.0\n",
    "labels = np.array(labels)\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of image in training set  1538 1538\n",
      "Number of image in tesing set  385 385\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# partition the data into training and testing splits using 75% of\n",
    "# the data for training and the remaining 25% for testing\n",
    "(trainX, testX, trainY, testY) = train_test_split(data, labels, test_size=0.2, random_state=RANDOM_STATE)\n",
    "\n",
    "print(\"Number of image in training set \", len(trainX), len(trainY))\n",
    "print(\"Number of image in tesing set \", len(testX), len(testY))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainY = to_categorical(trainY, num_classes=len(class_dict))\n",
    "testY = to_categorical(testY, num_classes=len(class_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         (None, 48, 48, 3)         0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 48, 48, 64)        1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 48, 48, 64)        36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 24, 24, 64)        0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 24, 24, 128)       73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 24, 24, 128)       147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 12, 12, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 12, 12, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 12, 12, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 12, 12, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 6, 6, 256)         0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 6, 6, 512)         1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 6, 6, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 6, 6, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 3, 3, 512)         0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 3, 3, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 3, 3, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 3, 3, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 1, 1, 512)         0         \n",
      "=================================================================\n",
      "Total params: 14,714,688\n",
      "Trainable params: 0\n",
      "Non-trainable params: 14,714,688\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\n",
    "base_model = VGG16(\n",
    "    weights='imagenet',\n",
    "    include_top=False,\n",
    "    input_shape=(IMAGE_DIM, IMAGE_DIM, 3))\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "base_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = Adam(lr=INIT_LR, decay=INIT_LR / EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Building model based on Function API\n",
    "X = base_model.output\n",
    "X = Flatten()(X)  \n",
    "\n",
    "#Adding last FC layers based on hyperparams given\n",
    "\n",
    "X = Dense(128, activation='relu')(X)\n",
    "X = Dense(128, activation='relu')(X)\n",
    "\n",
    "# for layer_param in hyperparameters['top_layers']:\n",
    "# \tX = self.layers[layer_param[0]](layer_param[1], activation=layer_param[2])(X)\n",
    "\n",
    "#Adding dropout\n",
    "X = Dropout(0.5)(X)\n",
    "\n",
    "#Adding the last layer for prediction\n",
    "predictions = Dense(7, activation='softmax')(X)\n",
    "\n",
    "#creating the final model\n",
    "model = Model(base_model.input, predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=[\"categorical_accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1538 samples, validate on 385 samples\n",
      "Epoch 1/15\n",
      "1538/1538 [==============================] - 63s 41ms/step - loss: 1.8112 - categorical_accuracy: 0.2555 - val_loss: 1.6374 - val_categorical_accuracy: 0.3714\n",
      "Epoch 2/15\n",
      "1538/1538 [==============================] - 59s 38ms/step - loss: 1.5834 - categorical_accuracy: 0.3791 - val_loss: 1.4006 - val_categorical_accuracy: 0.4494\n",
      "Epoch 3/15\n",
      "1538/1538 [==============================] - 59s 38ms/step - loss: 1.3816 - categorical_accuracy: 0.4642 - val_loss: 1.1992 - val_categorical_accuracy: 0.5377\n",
      "Epoch 4/15\n",
      "1538/1538 [==============================] - 59s 39ms/step - loss: 1.1661 - categorical_accuracy: 0.5592 - val_loss: 1.1338 - val_categorical_accuracy: 0.5688\n",
      "Epoch 5/15\n",
      "1538/1538 [==============================] - 63s 41ms/step - loss: 1.0034 - categorical_accuracy: 0.6216 - val_loss: 0.9826 - val_categorical_accuracy: 0.6364\n",
      "Epoch 6/15\n",
      "1538/1538 [==============================] - 59s 38ms/step - loss: 0.8554 - categorical_accuracy: 0.6736 - val_loss: 0.7563 - val_categorical_accuracy: 0.6883\n",
      "Epoch 7/15\n",
      "1538/1538 [==============================] - 59s 38ms/step - loss: 0.7149 - categorical_accuracy: 0.7464 - val_loss: 0.6450 - val_categorical_accuracy: 0.7740\n",
      "Epoch 8/15\n",
      "1538/1538 [==============================] - 60s 39ms/step - loss: 0.6581 - categorical_accuracy: 0.7477 - val_loss: 0.5766 - val_categorical_accuracy: 0.8000\n",
      "Epoch 9/15\n",
      "1538/1538 [==============================] - 59s 38ms/step - loss: 0.5478 - categorical_accuracy: 0.7913 - val_loss: 0.4560 - val_categorical_accuracy: 0.8597\n",
      "Epoch 10/15\n",
      "1538/1538 [==============================] - 59s 38ms/step - loss: 0.4858 - categorical_accuracy: 0.8225 - val_loss: 0.5014 - val_categorical_accuracy: 0.8078\n",
      "Epoch 11/15\n",
      "1538/1538 [==============================] - 59s 38ms/step - loss: 0.3966 - categorical_accuracy: 0.8544 - val_loss: 0.3838 - val_categorical_accuracy: 0.8649\n",
      "Epoch 12/15\n",
      "1538/1538 [==============================] - 59s 38ms/step - loss: 0.3859 - categorical_accuracy: 0.8563 - val_loss: 0.2963 - val_categorical_accuracy: 0.8883\n",
      "Epoch 13/15\n",
      "1538/1538 [==============================] - 59s 38ms/step - loss: 0.3108 - categorical_accuracy: 0.8973 - val_loss: 0.3930 - val_categorical_accuracy: 0.8468\n",
      "Epoch 14/15\n",
      "1538/1538 [==============================] - 59s 38ms/step - loss: 0.3457 - categorical_accuracy: 0.8726 - val_loss: 0.2426 - val_categorical_accuracy: 0.9273\n",
      "Epoch 15/15\n",
      "1538/1538 [==============================] - 60s 39ms/step - loss: 0.2694 - categorical_accuracy: 0.9103 - val_loss: 0.2672 - val_categorical_accuracy: 0.8987\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7ff543e20400>"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(trainX, trainY, epochs=15, batch_size=10, validation_data=(testX, testY))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "aug = ImageDataGenerator(\n",
    "    rotation_range=30,\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode=\"nearest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1538/1538 [==============================] - 45s 30ms/step\n",
      "\n",
      "categorical_accuracy: 93.63%\n",
      "385/385 [==============================] - 11s 30ms/step\n",
      "\n",
      "categorical_accuracy: 89.87%\n"
     ]
    }
   ],
   "source": [
    "# evaluate the model : train\n",
    "scores = model.evaluate(trainX, trainY)\n",
    "print(\"\\n%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\n",
    "\n",
    "\n",
    "# evaluate the model : test\n",
    "scores = model.evaluate(testX, testY)\n",
    "print(\"\\n%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('keras_model')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=load_model('keras_model')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Show some emotion and press enter to continue\n"
     ]
    }
   ],
   "source": [
    "camera = cv2.VideoCapture(0)\n",
    "input(\"Show some emotion and press enter to continue\")\n",
    "return_value, image = camera.read()\n",
    "cv2.imwrite('opencv.jpg', image)\n",
    "del(camera)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = cv2.imread('surprise.jpeg')\n",
    "img_gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "orig = image.copy()\n",
    "        \n",
    "# pre-process the image for classification\\n\n",
    "image = cv2.resize(image, (IMAGE_DIM, IMAGE_DIM))\n",
    "image = image.astype(float) / 255.0\n",
    "image = img_to_array(image)\n",
    "image = np.expand_dims(image, axis=0)\n",
    "        # load the trained convolutional neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.6636047e-09,\n",
       " 2.600159e-07,\n",
       " 1.0144707e-06,\n",
       " 1.4753546e-05,\n",
       " 1.8351139e-07,\n",
       " 2.7482342e-06,\n",
       " 0.99998105)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(anger,contempt,disgust,fear,happy,sadness,surprise) = model.predict(image)[0]\n",
    "(anger,contempt,disgust,fear,happy,sadness,surprise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'surprise'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x=dict(zip(('anger','contempt','disgust','fear','happy','sadness','surprise'),(anger,contempt,disgust,fear,happy,sadness,surprise)))\n",
    "max_key = max(x, key=lambda k: x[k])\n",
    "max_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.remove('opencv.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
