{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Importing all the necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/home/tennyson/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/tennyson/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/tennyson/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/tennyson/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/tennyson/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/tennyson/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/home/tennyson/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/tennyson/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/tennyson/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/tennyson/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/tennyson/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/tennyson/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import random\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from imutils import paths\n",
    "import face_recognition\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Activation\n",
    "from keras.preprocessing.image import img_to_array\n",
    "from keras.utils import to_categorical\n",
    "from keras.optimizers import Adam, SGD\n",
    "from keras.callbacks import EarlyStopping, TensorBoard, ModelCheckpoint\n",
    "from keras import applications\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.applications.vgg19 import VGG19\n",
    "from keras.applications.resnet50 import ResNet50\n",
    "from keras.applications.inception_v3 import InceptionV3\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, Dropout, Flatten, Conv2D, MaxPooling2D\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.metrics import categorical_accuracy\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import load_model\n",
    "from keras import backend as K\n",
    "from collections import defaultdict\n",
    "\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report, precision_recall_fscore_support, average_precision_score, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Initializing parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "path='data'\n",
    "data = []\n",
    "labels = []\n",
    "features = []\n",
    "RANDOM_STATE = 42\n",
    "IMAGE_DIM = 48\n",
    "TEST_SPLIT = 0.20\n",
    "BATCH_SIZE = 50\n",
    "EPOCHS = 15\n",
    "INIT_LR = 1e-3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Reading images with different classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class information :  {'1': 0, '2': 1, '3': 2, '4': 3, '5': 4, '6': 5, '7': 6}\n"
     ]
    }
   ],
   "source": [
    "class_dir_paths = sorted(os.listdir(path))\n",
    "class_dict = {}\n",
    "idx = 0\n",
    "\n",
    "for class_dir_path in class_dir_paths:\n",
    "    if (os.path.isdir(os.path.join(path, class_dir_path))):\n",
    "        class_name = class_dir_path\n",
    "        class_dict[class_name] = idx\n",
    "        idx += 1\n",
    "\n",
    "print(\"Class information : \", class_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<module 'imutils.paths' from '/usr/local/lib/python3.6/dist-packages/imutils/paths.py'>\n"
     ]
    }
   ],
   "source": [
    "imagePaths = sorted(list(paths.list_images(path)))\n",
    "random.seed(RANDOM_STATE)\n",
    "random.shuffle(imagePaths)\n",
    "print(paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of images readed,  1913\n",
      "Total number of labels extracted,  1913\n"
     ]
    }
   ],
   "source": [
    "# loop over the input images\n",
    "for imagePath in imagePaths:\n",
    "    # load the image, pre-process it, and store it in the data list\n",
    "    image = cv2.imread(imagePath)\n",
    "    face_locations = face_recognition.face_locations(image)\n",
    "    i = face_locations[0]\n",
    "    a = min(i[0],i[2])\n",
    "    b = max(i[0],i[2])\n",
    "    c = min(i[1],i[3])\n",
    "    d = max(i[1],i[3])\n",
    "#     image = cv2.rectangle(image,(i[0],i[1]),(i[2],i[3]), color, thickness) \n",
    "    image = image[a:b,c:d]\n",
    "    image = cv2.resize(image, (IMAGE_DIM, IMAGE_DIM))\n",
    "    image = img_to_array(image)\n",
    "    data.append(image)\n",
    "    # extract the class label from the image path and update the\n",
    "    # labels list\n",
    "    label = imagePath.split(os.path.sep)[-2]\n",
    "    labels.append(class_dict[label])\n",
    "\n",
    "print(\"Total number of images readed, \", len(data))\n",
    "print(\"Total number of labels extracted, \", len(labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Preprocessing image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1913, 48, 48, 3)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = np.array(data, dtype=\"float\")/255.0\n",
    "labels = np.array(labels)\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Splitting data into train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of image in training set  1530 1530\n",
      "Number of image in tesing set  383 383\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# partition the data into training and testing splits using 75% of\n",
    "# the data for training and the remaining 25% for testing\n",
    "(trainX, testX, trainY, testY) = train_test_split(data, labels, test_size=0.2, random_state=RANDOM_STATE)\n",
    "\n",
    "print(\"Number of image in training set \", len(trainX), len(trainY))\n",
    "print(\"Number of image in tesing set \", len(testX), len(testY))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainY = to_categorical(trainY, num_classes=len(class_dict))\n",
    "testY = to_categorical(testY, num_classes=len(class_dict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Deep Learning using Transfer Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"vgg16\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         (None, 48, 48, 3)         0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 48, 48, 64)        1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 48, 48, 64)        36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 24, 24, 64)        0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 24, 24, 128)       73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 24, 24, 128)       147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 12, 12, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 12, 12, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 12, 12, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 12, 12, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 6, 6, 256)         0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 6, 6, 512)         1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 6, 6, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 6, 6, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 3, 3, 512)         0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 3, 3, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 3, 3, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 3, 3, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 1, 1, 512)         0         \n",
      "=================================================================\n",
      "Total params: 14,714,688\n",
      "Trainable params: 0\n",
      "Non-trainable params: 14,714,688\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\n",
    "base_model = VGG16(\n",
    "    weights='imagenet',\n",
    "    include_top=False,\n",
    "    input_shape=(IMAGE_DIM, IMAGE_DIM, 3))\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "base_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = Adam(lr=INIT_LR, decay=INIT_LR / EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Building model based on Function API\n",
    "X = base_model.output\n",
    "X = Flatten()(X)  \n",
    "\n",
    "#Adding last FC layers based on hyperparams given\n",
    "\n",
    "X = Dense(64, activation='relu')(X)\n",
    "# X = Dense(128, activation='relu')(X)\n",
    "\n",
    "# for layer_param in hyperparameters['top_layers']:\n",
    "# \tX = self.layers[layer_param[0]](layer_param[1], activation=layer_param[2])(X)\n",
    "\n",
    "#Adding dropout\n",
    "X = Dropout(0.5)(X)\n",
    "\n",
    "#Adding the last layer for prediction\n",
    "predictions = Dense(7, activation='softmax')(X)\n",
    "\n",
    "#creating the final model\n",
    "model = Model(base_model.input, predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tennyson/.local/lib/python3.6/site-packages/ipykernel_launcher.py:21: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(activation=\"softmax\", units=7)`\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv2D(32, (3, 3), input_shape=(IMAGE_DIM, IMAGE_DIM,3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D())\n",
    "\n",
    "model.add(Conv2D(32, (3, 3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D())\n",
    "\n",
    "model.add(Conv2D(64, (3, 3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D())\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(output_dim = 7, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=[\"categorical_accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fitting model with the data \n",
    "# model.fit(trainX, trainY, epochs=15, batch_size=10, validation_data=(testX, testY))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data augmentation for further imrpovements but not used so far\n",
    "datagen = ImageDataGenerator(\n",
    "    rotation_range=30,\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode=\"nearest\")\n",
    "datagen.fit(trainX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00011: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x7f9207fe36d8>"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1)\n",
    "model.fit_generator(datagen.flow(trainX, trainY, batch_size=BATCH_SIZE),\n",
    "                   epochs=EPOCHS, validation_data=(testX, testY),verbose=0, callbacks=[es])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 0.640, Test: 0.671\n"
     ]
    }
   ],
   "source": [
    "_, train_acc = model.evaluate(trainX, trainY, verbose=0)\n",
    "_, test_acc = model.evaluate(testX, testY, verbose=0)\n",
    "print('Train: %.3f, Test: %.3f' % (train_acc, test_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1530/1530 [==============================] - 5s 3ms/step\n",
      "\n",
      "categorical_accuracy: 63.99%\n",
      "383/383 [==============================] - 1s 3ms/step\n",
      "\n",
      "categorical_accuracy: 67.10%\n"
     ]
    }
   ],
   "source": [
    "# evaluate the model : train\n",
    "scores = model.evaluate(trainX, trainY)\n",
    "print(\"\\n%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\n",
    "\n",
    "\n",
    "# evaluate the model : test\n",
    "scores = model.evaluate(testX, testY)\n",
    "print(\"\\n%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('keras_model') #save model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=load_model('keras_model') #load model without training model again\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Expression prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Show some emotion and press enter to continue\n"
     ]
    }
   ],
   "source": [
    "camera = cv2.VideoCapture(0)\n",
    "input(\"Show some emotion and press enter to continue\")\n",
    "return_value, image = camera.read()\n",
    "cv2.imwrite('opencv.jpg', image)\n",
    "del(camera)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "image = cv2.imread('opencv.jpg')\n",
    "face_locations = face_recognition.face_locations(image)\n",
    "i = face_locations[0]\n",
    "a = min(i[0],i[2])\n",
    "b = max(i[0],i[2])\n",
    "c = min(i[1],i[3])\n",
    "d = max(i[1],i[3])\n",
    "#     image = cv2.rectangle(image,(i[0],i[1]),(i[2],i[3]), color, thickness) \n",
    "image = image[a:b,c:d]       \n",
    "# pre-process the image for classification\\n\n",
    "image = cv2.resize(image, (IMAGE_DIM, IMAGE_DIM))\n",
    "plt.imshow(image)\n",
    "image = image.astype(float) / 255.0\n",
    "image = img_to_array(image)\n",
    "image = np.expand_dims(image, axis=0)\n",
    "#         load the trained convolutional neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.28064045,\n",
       " 0.2104858,\n",
       " 0.048251748,\n",
       " 0.056567922,\n",
       " 0.02441036,\n",
       " 0.09274004,\n",
       " 0.28690368)"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(anger,contempt,disgust,fear,happy,sadness,surprise) = model.predict(image)[0]\n",
    "(anger,contempt,disgust,fear,happy,sadness,surprise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'surprise'"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x=dict(zip(('anger','contempt','disgust','fear','happy','sadness','surprise'),(anger,contempt,disgust,fear,happy,sadness,surprise)))\n",
    "max_key = max(x, key=lambda k: x[k])\n",
    "max_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.remove('opencv.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
